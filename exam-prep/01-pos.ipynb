{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n",
    "## Intro: What is a word?\n",
    "* A Word: Smallest semantic unit\n",
    "* Morphology: structure of sentences made up of words\n",
    "* Morpheme: smallest meaningful unit of a word, such as\n",
    "    * Stems: The core of the word\n",
    "    * Affixes: AddOns such as \"un\"-attractive, run-\"s\"\n",
    "    \n",
    "\n",
    "## Word Classes aka Parts of Speech (PoS)\n",
    "* Tell us about neighbouring words\n",
    "* Gives syntactic structure to language\n",
    "* Useul for information extraction and speech recognition\n",
    "\n",
    "Examples:\n",
    "* Noun\n",
    "* Verb\n",
    "* Adj\n",
    "* Adv: degree of something ex \"slowly\", \"luckily\"\n",
    "* Preposition\n",
    "* Pronoun: I, me, mine, he, her\n",
    "* Determiner: The, a, that, those\n",
    "\n",
    "### Open and Closed classes\n",
    "Closed - established:\n",
    "* Prepositions\n",
    "* Pronouns\n",
    "* Conjunctions\n",
    "* Particles\n",
    "\n",
    "Open - constantly growing:\n",
    "* Nouns\n",
    "* Verbs\n",
    "* Adjectives\n",
    "* Adverbs\n",
    "\n",
    "### Ambiguity\n",
    "Some words can belong to different word classes depending on the context, ex \n",
    "* The dogs bite the cat\n",
    "\n",
    "Bite can be either verb or noun (here verb)\n",
    "\n",
    "### Penn-Treebank\n",
    "Complete database and tagset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM PoS Tagger\n",
    "Let a HMM be defined by following parameters\n",
    "\n",
    "$$HMM = (Q, O, A, B)$$\n",
    "\n",
    "* States: $Q = q_1 ... q_N$ [parts of speech]\n",
    "* Observations: $O = o_1 ... o_V$ [words]\n",
    "* Transition matrix: $A = a_{ij} = P(y_t = q_j \\vert y_{t-1} = q_i)$\n",
    "* Emission matrix: $B = b_{ik} = P(x_t = o_k \\vert y_t = q_i)$\n",
    "\n",
    "### Markov Chain\n",
    "We modelling tagging words as PoS as a Hidden Markov Process\n",
    "\n",
    "We rely on 2 indenpendence assumptions:\n",
    "\n",
    "__Markov Independence__\n",
    "$$P(q_i \\vert q_1 ... q_{i-1}) \\approx P(q_i \\vert q_{i-1})$$\n",
    "* Current state depends only on the previous state\n",
    "\n",
    "__Output Independence__\n",
    "$$P(o_i \\vert q_1 ... q_i ... q_T, o_1 ... o_i ... o_T) = P(o_i \\vert q_i)$$\n",
    "* Current observation only depends on the state that created it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative process\n",
    "Likelihood: $$P(O \\vert Q; \\theta) = \\prod_t^T P(q_i \\vert q_{i-1}) P(o_i \\vert q_i)$$\n",
    "* $\\theta = (A,B)$\n",
    "\n",
    "### Inference with HMMs\n",
    "* Given $O$ compute $P(O)$\n",
    "* Given $O$ compute most likely sequence of POS $Q=q_1...q_N$\n",
    "* $Q = q_i...q_N$ estimate params $\\theta = (A,B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood of Observation Sequence\n",
    "### Observation Probability\n",
    "Sum over sequence $Q$, use likelihood $P(O \\vert Q; \\theta)$.\n",
    "\n",
    "$$P(O; \\theta) = \\sum_{q \\in Q} P(O \\vert Q; \\theta)$$\n",
    "\n",
    "Problem: $N^T$ possible hidden sequences for $N$ hidden states and $T$ sequences, this is infeasible for large values.\n",
    "\n",
    "### Forward Algorithm - Compute Likelihood of Observed Seq\n",
    "The problem can be solved with dynamic programming, i.e. storing intermediate results to avoid computing them over and over.\n",
    "\n",
    "Use forward trellis $\\alpha$, where $\\alpha_t(j)$ is the probability of ending up in state $j$ after seeing the first $t$ observations given the automation $\\lambda = (A, B)$\n",
    "\n",
    "$$\\alpha_t (j) = P(o_1...o_t, q_t=j \\vert \\lambda)$$\n",
    "\n",
    "We compute $\\alpha_t (j)$ by summing over the extensions of all paths leading to the current cell.\n",
    "\n",
    "Let:\n",
    "* $\\alpha_{t-1} (i)$: The previous forward path prob from previous time step\n",
    "* $a_{ij} = p(q_i | q_j)$: Transition probability from state $q_i$ to $q_j$\n",
    "* $b_j(o_t) = p(o_t | q_j)$: Likelihood of observing $o_t$ given current state $j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Trellis\n",
    "Deriving an expression for $\\alpha$ which allows dynamic programming using the chain rule:\n",
    "\n",
    "$\\alpha^t (j) = P(o^{1:t}, q^t = q_j)$\n",
    "* $=\\sum_{i=1}^N p(o^{1:t-1}, o^{t}, q^t = q_j, q^{t-1} = q_i)$\n",
    "\n",
    "* $=\\sum_{i=1}^N p(o^{t} | o^{1:t-1}, q^t = q_j, q^{t-1} = q_i) \\cdot p(o^{1:t-1}, q^t = q_j, q^{t-1} = q_i)$\n",
    "\n",
    "* $=\\sum_{i=1}^N p(o^{t} | o^{1:t-1}, q^t = q_j, q^{t-1} = q_i) \\cdot p(q^t = q_j | o^{1:t-1}, q^{t-1} = q_i) \\cdot p(o^{1:t-1}, q^{t-1} = q_i)$\n",
    "\n",
    "Since the current state only depends on the previous state and an obs. only depends on the corresponding hidden state, we can re-write a couple of terms:\n",
    "\n",
    "* $p(o^{t} | o^{1:t-1}, q^t = q_j, q^{t-1} = q_i) =  p(o^{t} | q^t = q_j)  = b_j(o^t)$ [output indep.]\n",
    "\n",
    "* $p(q^t = q_j | o^{1:t-1}, q^{t-1} = q_i) =  p(q^t = q_j |Â q^{t-1} = q_i) = a_{ij}$ [markov indep. and def of A]\n",
    "\n",
    "By definition of $\\alpha$, we can re-write the following term to be the previous iteration.\n",
    "* $p(o^{1:t-1}, q^{t-1} = q_i) = \\alpha^{t-1}$\n",
    "\n",
    "Using the 3 terms again we achieve:\n",
    "$$\\alpha^t(j) = \\sum_{i=1}^N b_j(o^t) \\cdot a_{ij} \\cdot \\alpha^{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Algorithm Steps\n",
    "The algorithm uses 3 steps:\n",
    "1. Init step\n",
    "$$\\alpha_1 = \\pi_j b_j(o_1) \\quad 1\\leq j \\leq N$$\n",
    "\n",
    "2. Recursion step\n",
    "$$\\alpha_t (j) = \\sum_{i=1}^N \\alpha_{t-1} (i) a_{ij} b_j(o_t) \\quad 1\\leq j \\leq N, 1 < t \\leq T$$\n",
    "\n",
    "3. Termination step (sum over alpha for last timestep)\n",
    "$$P(O \\vert \\lambda) = \\sum_{i=1}^N \\alpha_T (i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "__Decoding__\n",
    "* Given $\\lambda = (A,M)$ and $O= o_1 ... o_T$ find the most probably sequence of hidden states $Q = q_1 ... q_T$\n",
    "\n",
    "Example: \n",
    "* Weather is the hidden variable, and number of ice creams eaten on a day is the observed variable.\n",
    "* __Decoding__ is then to find the most likely weather sequence - given the ice cream sequence.\n",
    "\n",
    "### Viterbi algorithm - Compute Most Probable Hidden Seq\n",
    "Algorithm for decoding\n",
    "* Also makes use of a dynamic programming trellis.\n",
    "* Process the observed sequence left to right, in order to fill out trellis\n",
    "\n",
    "$$v_t(j) = max_i \\quad v_{t-1} (i) a_{ij} b_j(o_t) \\quad i=1...N$$\n",
    "\n",
    "The trellis entry $v_t(j)$ is the prob. of HMM being in state $j$ after seeing the first $t$ observations, where it has passed through the most probable sequence $q_1 ...q_{t-1}$.\n",
    "\n",
    "* v_{t-1}(i): Previous viterbi path probability from prev step\n",
    "* $a_{ij}$: Transition probability from prev. state $q_i$ to $q_j$\n",
    "* $b_j(o_t)$: State observation likelihood for $o_t$ given current state $j$\n",
    "\n",
    "Similar to the forward algorithm, except for:\n",
    "* Uses the __max__ over previous path probabilities, forward algo uses the __sum__.\n",
    "* Backpointers: In order to produce an observation likelihood we have to keep track of the path which lead to each state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm steps:\n",
    "\n",
    "1. Init\n",
    " * Init trellis: $v_1(j) = \\pi_j b_j(o_1)$\n",
    " * Init emission matrix: $b_{t_1}(j) = 0 $\n",
    " * Where $1 \\leq j \\leq N$\n",
    " \n",
    "2. Recursion\n",
    " * Update trellis: $v_t(j) = max \\ v_{t-1}(i) a_{ij} b_j(o_t)$\n",
    " * Update emission matrix: $b_{t_t}(j) = argmax_i \\ v_{t-1} (i) a_{ij} b_j(o_t)$\n",
    " * Where $1 \\leq j \\leq N$ and $1 < t \\leq T$\n",
    " \n",
    "3. Termination\n",
    " * Highest probability: $P^* = max \\ v_T (i)$\n",
    " * Backtrace: $q_T^* = argmax \\ v_T(i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-Backward Algorithm - Learn Params\n",
    "Goal: Learn $\\lambda = (A,B)$ given $O$ and a set of _possible_ states in the HMM, $Q$. This is done through expectation maximization.\n",
    "\n",
    "Done iteratively through Baum-Welch, EM algorithm.\n",
    "\n",
    "For PoS Tagging this is actually unnecessary since our hidden states are in fact observed! We have PoS tags for our words through the Penn Tree Bank\n",
    "\n",
    "#### Example\n",
    "Toy example for ice cream: \n",
    "* $Q = \\{q_1, q_2\\} = \\{hot, cold\\}$\n",
    "* $O = [o_1, o_2, o_3] = [1, 2, 3]$\n",
    "\n",
    "If given observation sequences matched with hidden state sequences then we can compute:\n",
    "\n",
    "* $\\pi_{q_1}$ and $\\pi_{q_2}$ by simply counting how many start in the two state\n",
    "* Compute A from the transitions, ex.\n",
    "    * $p(q_1|q_1)$\n",
    "    * $p(q_1|q_2)$\n",
    "    * $p(q_2|q_2)$\n",
    "    * $p(q_2|q_1)$\n",
    "* Compute B matrix:\n",
    "    * $p(o_1 | q_1)$, $p(o_1 | q_2)$\n",
    "    * $p(o_2 | q_1)$, $p(o_2 | q_2)$\n",
    "    * $p(o_3 | q_1)$, $p(o_3 | q_2)$\n",
    "\n",
    "However we do not known the counts for being in any of the hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction\n",
    "Who did what to whom and why?\n",
    "\n",
    "### Named Entity Recognition\n",
    "Task: Detecting spans of text which constitute a named entity in the text\n",
    "\n",
    "Named entity: Person/location/organisation which is named\n",
    "* Can be extended to entities such as prices, dates, times\n",
    "\n",
    "### Sequence Labeling\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
