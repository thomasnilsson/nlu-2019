{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation\n",
    "\n",
    "How to automate translation between languages?\n",
    "\n",
    "Notation\n",
    "* Translate from: $S = source$ sometimes\n",
    "    * Sometimes denoted $F=french$\n",
    "* Translate to $T=target$\n",
    "    * Sometiems denoted $E=english$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Word Models\n",
    "The chosen english sentence is the one which is most probable, given the foreign sentence, i.e.\n",
    "\n",
    "$$\\hat{e} = argmax_e \\ P(e|f)$$\n",
    "\n",
    "This can be re-written using Bayes Rule: $p(e|f) = \\frac{p(f|e) p(e)}{p(f)}$\n",
    "\n",
    "$$\\hat{e} = argmax_e \\ \\frac{p(f|e) p(e)}{p(f)}$$\n",
    "\n",
    "\n",
    "We can ignore $P(f)$ since this will be constant across all english sentences, and thus is not relevant to consider when designing the objective function. Thus the most probable english sentence becomes \n",
    "$$\\hat{e}= argmax_e \\ P(f|e) P(e)$$\n",
    "\n",
    "* $P(f|e)$: Translation model\n",
    "* $P(e)$: Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Model 1\n",
    "\n",
    "Translation model $$p(f|e) = \\sum_a P(f,a|e)$$\n",
    "* French sentence $f = (f_1 ... f_{m})$\n",
    "* English sentence $e = (e_1 ... f_{n})$\n",
    "* Alignment: $a = (a_1 ... a_m)$\n",
    "\n",
    "__Alignment__ \n",
    "\n",
    "Alignment function $a: j\\rightarrow i$\n",
    "* Map target word $w_j$ to source word $w_i$\n",
    "* Keep track of alignments when translating\n",
    "* Also words when one word in $S$ becomes multiple words in $T$\n",
    "* Some words may also be dropped or added in translation\n",
    "\n",
    "$$P(f,a | e) = P(f| e, a) \\cdot P(a|e) \\quad \\text{[Chain rule]}$$ \n",
    "\n",
    "$$P(f,a | e) = \\frac{\\epsilon}{(n+1)^m} \\prod_{j=1}^m t(f_j | e_{a(j)})$$\n",
    "* $t(f_j, e_i) = P(f_j | e_i)$: Probability of trainslating $e_i$ as $f_j$\n",
    "* $\\epsilon$: Normalization constant\n",
    "* $(n+1)^m$: Number of possible alignments\n",
    "\n",
    "__Training with EM Algorithm__\n",
    "* Input: sentence aligned corpus of $N$ sentences\n",
    "    * Uniform $t(f_j | e_i)$ distr.\n",
    "\n",
    "E-Step: Apply model on the data\n",
    "$$E[count(f_j, e_i)] = \\sum_{(f,e)} p(a | f,e)$$\n",
    "$$P(a | f,e) = \\frac{P(f, a, | e)}{\\sum_a P(f,a | e)}$$\n",
    "\n",
    "M-Step: Normalize probability\n",
    "$$t(f_j | e_i) \\frac{E[count(f_j, e_i)]}{\\sum_j E[count(f_j, e_i)]}$$\n",
    "\n",
    "__Evaluate model__\n",
    "\n",
    "Perplexity score: How well the model fits the data\n",
    "$$log_2 Perp = - \\sum_s log_2 \\ P(f|e)$$\n",
    "\n",
    "Limitations of IBM models:\n",
    "* Word alignments allow many-to-one\n",
    "* ... But not one-to-many or many-to-many!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Based Model\n",
    "\n",
    "Unit of translation: Whole phrase\n",
    "* Allows many-to-many modelling, carries more context, improves with more data\n",
    "* State of the art before Deep Learning Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU - Translation Evalutation\n",
    "Evaluation of translation is a hard, subjective problem - very human.\n",
    "\n",
    "Idea: Simple precision score ~ predicted n-grams frequency in references\n",
    "\n",
    "$$P_n = \\frac{\\text{#correct n-grams}}{\\text{#total n-grams}} $$\n",
    "\n",
    "$$BLEU = \\text{min} \\ (1, \\frac{\\text{len(output}}{\\text{len(reference)}}) \\cdot (\\prod_{n=1}^4 P_n)^{1/4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Reference text:\n",
    "* `Israeli officials are responsible for airport security`\n",
    "\n",
    "Model A:\n",
    "* `Israeli officials reponsibility of airport safety`\n",
    "* 1-gram: 3/6\n",
    "* 2-gram: 1/5\n",
    "* 3-gram: 0/4\n",
    "* 4-gram: 0/3\n",
    "* Brevity: 6/7\n",
    "\n",
    "Model B:\n",
    "* `airport security Israeli officials are responsible`\n",
    "* 1-gram: 6/6\n",
    "* 2-gram: 4/5\n",
    "* 3-gram: 2/4\n",
    "* 4-gram: 1/3\n",
    "* Brevity: 6/7\n",
    "\n",
    "$$BLEU_A = min(1, \\frac{6}{7}) \\cdot (\\frac{3}{6} \\frac{1}{5} \\frac{0}{4} \\frac{0}{3})^{1/4} = 0$$\n",
    "\n",
    "$$BLEU_B = min(1, \\frac{6}{7}) \\cdot (\\frac{6}{6} \\frac{4}{5} \\frac{2}{4} \\frac{1}{3})^{1/4} = 0.518$$\n",
    "\n",
    "So, Model B performs vastly better than A wrt. BLEU, even though the word odering is a very yoda-like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
