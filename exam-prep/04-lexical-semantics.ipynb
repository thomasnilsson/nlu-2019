{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Semantics\n",
    "\n",
    "Representing the __meaning of a word__\n",
    "\n",
    "* Ex cat is similar to dog\n",
    "\n",
    "## Words and Vectors\n",
    "### Word and Vector Similarity \n",
    "Most words dont have synonyms, but many similar/related words\n",
    "* Glad, happy, joyous, exstatic\n",
    "\n",
    "Idea: Project word vector onto some space where similar words have a smaller distance between them than unrelated words\n",
    "\n",
    "Dot product: \n",
    "* > 0: The higher the dot product, the smaller the angle between them and the more similar they are.\n",
    "* = 0: Orthogonal vectors\n",
    "* < 0: Angle is larger than 90 degrees, words are likely very opposite.\n",
    "\n",
    "Ex cosine similarity\n",
    "\n",
    "#### Term-Document Matrix\n",
    "* Columns: Documents, ex books, articles\n",
    "* Rows: Words\n",
    "* $M_{ij}$: Word count for $w_i$ in document $d_j$\n",
    "\n",
    "This allows us to represent documents in a vector space, and the similarity between documents is then the word counts. \n",
    "\n",
    "#### Co-occurence\n",
    "You shall know a word by the company it keeps\n",
    "\n",
    "Look at co-occurence of word in shared contexts\n",
    "\n",
    "Often you can tell what an unknown word means, just by looking at the contexts it appears in.\n",
    "\n",
    "````\n",
    "A bottle of X is on the table \n",
    "Everybody likes X\n",
    "X makes you drunk\n",
    "We make X Ìˆino out of corn\n",
    "```\n",
    "\n",
    "We can infer that X is some kind of alcoholic drink, similar to beer.\n",
    "\n",
    "\n",
    "#### Association by Co-occurence\n",
    "Syntagmatic assocation: Words which occur in the same context\n",
    "* Wrote ~ {author, book, down, pen...}\n",
    "* First order co-occurence, since the words in fact appear together\n",
    "\n",
    "Paradigmatic association: Words which can be substituted for eachother\n",
    "* Monday ~ {Tuesday, Wednesday...}\n",
    "* Man ~ {Guy, Dude, Boy}\n",
    "* Second order co-occurence, since they mean almost the same thing and therefore often dont appear together in a context, but the contexts they occur in are similar!\n",
    "\n",
    "#### Co-occurence Matrix\n",
    "$N \\in I^{m\\times m}$ \n",
    "* Counts, many entries 0 (sparse)\n",
    "* $n_{ij}$ is the #contexts of $w_i$ for which $w_j$ occurs.\n",
    "\n",
    "Raw counts are not a good measure of assocation\n",
    "* Stop words co-occur with all other words\n",
    "\n",
    "Alternative: PMI\n",
    "\n",
    "_How often two events $x$ and $y$ occur compared to what we expect if they were independent._\n",
    "\n",
    "$$PMI(x,y) = log_2 \\ \\frac{P(x,y)}{P(x)P(y)}$$\n",
    "\n",
    "### Word2Vec\n",
    "Represent word with a fixed length, non-sparse vector, condense the information\n",
    "\n",
    "\n",
    "#### Skip-Gram Model with Negative Sampling\n",
    "How do we come up with a representation? \n",
    "\n",
    "Instead of counting how often each word $w$ occurs near $w'$ instead train a classifier on the following binary prediction task:\n",
    "* Is word $w$ likely to show up near $w'$? \n",
    "\n",
    "Concretely:\n",
    "* Treat target word and neighbouring context as positive examples (y=1)\n",
    "* Randomly sample other words in vocab to get negative samples (y=0)\n",
    "* Use logistic regression to train classifier - learn to distinguish between positive and negative\n",
    "* Use the learned weights as word embeddings\n",
    "\n",
    "So, rather than using the logistic regression model itself, we use the learned weights of the classifier - these are the word embeddings.\n",
    "\n",
    "Context window example: the two previous words and the next 2 words, ie.\n",
    "* $w_1 \\ w_2 \\ \\mathbf{w_3} \\ w_4 \\ w_5$\n",
    "* Context is $w_1 \\ w_2$ and $w_4 \\ w_5$\n",
    "\n",
    "\n",
    "### GloVe\n",
    "Alternative to negative sampling\n",
    "\n",
    "Idea: Weighted least squares fit for log-counts\n",
    "\n",
    "Introduce dampening function $f$:\n",
    "$$f(n) = min \\ (1, (\\frac{n}{n_{max}})^\\alpha) \\quad \\alpha \\in [0,1]$$\n",
    "\n",
    "Often $\\alpha = 3/4$\n",
    "\n",
    "This function limits the influence of very frequent words, and clips at a frequency $n_{max}$\n",
    "\n",
    "\n",
    "\n",
    "$$H = \\sum_{i,j} f(n_{ij}) (log \\ n_{ij} - log \\tilde{p}_\\theta (w_j | w_i))^2$$\n",
    "\n",
    "* Target: $log \\ n_{ij}$\n",
    "* Model: $log \\tilde{p}_\\theta (w_j | w_i)$\n",
    "* Weighting function: $f(n_{ij})$\n",
    "\n",
    "Is equivalent to matrix decomposition problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
