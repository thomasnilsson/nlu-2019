{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Grammar\n",
    "## Parsing\n",
    "Sentence $\\rightarrow$ components of sentence\n",
    "* Input: sentence\n",
    "* Ouput: parse tree with a PoS for each word in sentence\n",
    "    * Explains the \"who did what to whom and why?\"\n",
    "\n",
    "\n",
    "## Grammar\n",
    "The syntatic structure of a language\n",
    "\n",
    "* Phrase: meaningful unit words\n",
    "* Clasue: subject + predicate + phrases\n",
    "* Sentence: main verb + one or more clasuses\n",
    "\n",
    "### Context Free Grammar (CFG)\n",
    "Defined as $G = (N, \\Sigma, R, S)$\n",
    "* N: Non-terminals\n",
    "* $\\Sigma$: Terminals\n",
    "* R: Rules for language, ex $A_i \\rightarrow B_1 \\ B_2 \\ ... \\ B_N \\quad B_i \\in \\{N \\cup \\Sigma \\}$\n",
    "* S: Start symbol, $S \\in N$\n",
    "\n",
    "Problem: CFGs can produce multiple valid parse trees - ambiguity problem.\n",
    "\n",
    "### CFG Parsing\n",
    "Task: assign a parse tree (derivation) to input strings, such that\n",
    "* the tree covers all and only the input elements and starts with $S$\n",
    "* several such trees might exist\n",
    "\n",
    "Parsing Strategies\n",
    "* Top-down: start with S, try to reach the terminals\n",
    "* Bottom-up: start from the terminal, try to reach S\n",
    "\n",
    "### Chomksy Normal Form (CNF)\n",
    "Transforms grammar into rules with at most two productions, i.e. rules should have the form $A \\rightarrow BC$.\n",
    "\n",
    "Tranforming a rule of the form $A \\rightarrow BCD$ is done as follows:\n",
    "* $A \\rightarrow X D$\n",
    "* $X\\rightarrow BC$\n",
    "\n",
    "This also applies to longer right hand sides, in this case we iterate until it has been binarized.\n",
    "\n",
    "Example: $A \\rightarrow BCDE$ is done as follows:\n",
    "* $A \\rightarrow X Y$\n",
    "* $X\\rightarrow BC$\n",
    "* $Y\\rightarrow DE$\n",
    "\n",
    "Rules which have 2 or 1 productions are left untouched and simply transferred directly to the new grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKY Recognition\n",
    "* Dynamic programming approach to building parse trees.\n",
    "* Requires all rules to be in Chomsky Normal Form\n",
    "    * This allows the algorithm to encode the parse tree as a two-dimensional array\n",
    "\n",
    "Algorithm\n",
    "```\n",
    "CKY(S, G) -> T\n",
    "    let N = len(S)\n",
    "    let T = [N][N]\n",
    "    \n",
    "    for j = 1...N:\n",
    "        \n",
    "        // FILL OUT DIAGONAL\n",
    "        for all A which satisfy (A -> S[j] in G):\n",
    "            T[j-1, j] = T[j-1,j] UNION {A}\n",
    "        \n",
    "        // FILL OUT SUPER-DIAGONAL\n",
    "        for i = j-2...0:\n",
    "            for k = i+1...j-1:\n",
    "                for all A which satisfy (A -> BC in G) \n",
    "                    and (B in T[i, k]) // left\n",
    "                    and (C in T[k,j]): // below\n",
    "                        T[i,j] = T[i,j] UNION {A}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CKY Parsing\n",
    "The above algorithm simply builds a table, but not actually the parse itself.\n",
    "In order to do so the algorithm needs to add two steps:\n",
    "* Each non terminal also stores which rules it was derived from\n",
    "* Allow multiple versions of the same non-terminal in the table\n",
    "\n",
    "Recovering the parse tree can be done by running the Viterbi algorithm on the table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Parsing (PCFG)\n",
    "Probabilistic version of CFG, each rule is assigned a probability - usually derived from empirical data.\n",
    "\n",
    "$$PCFG = (N, \\Sigma, R, S)$$\n",
    "* N: Set of non terminals\n",
    "* $\\Sigma$: Set of terminals\n",
    "* R: Set of rules/productions of the form $A \\rightarrow  \\beta [p]$\n",
    "    * A: non-terminal\n",
    "    * $\\beta$: String of symbols produced\n",
    "    * p: the probability of choosing B, given A, p($\\beta$|A)\n",
    "        * Must satisfy $\\sum_\\beta p(A\\rightarrow \\beta) = 1$\n",
    "* S: Start symbol\n",
    "\n",
    "#### Disambiguation with PCFGs\n",
    "Probabilities can be used to choose the most likely parse tree, by looking at the conditional probability for generating it.\n",
    "\n",
    "Let the i'th rule be defined as $LHS_i \\rightarrow RHS_i$.\n",
    "\n",
    "The probability of a parse tree $T$ which uses $n$ rules, given a sentence $S$ is then:\n",
    "\n",
    "$$P(T,S) \\prod_{i=1}^n P(RHS_i \\ | \\ LHS_i)$$\n",
    "\n",
    "$P(T,S)$ is both the joint prob. or the parse and setences - as well as the prob. of the parse $P(T)$.\n",
    "\n",
    "__Explanation__\n",
    "* Joint probability of T, S: $P(T,S) = P(S|T) \\cdot P(T) = P(T|S) \\cdot P(S)$\n",
    "\n",
    "* Since a parse tree $T$ includes all words in $S$, the probability $P(w_1, w_2, ..., w_n|T)$ is 1 - i.e. $P(S|T) = 1$.\n",
    "* This means $P(T,S) = P(S|T)\\cdot P(T) = 1 \\cdot P(T) = P(T)$\n",
    "\n",
    "__Disambiguation__\n",
    "\n",
    "Let the strings of S be called the yield of some parse tree T over S.\n",
    "\n",
    "Disambiguation is then picking the parse tree most probably given S:\n",
    "\n",
    "$$T^*(S) = argmax_T \\ P(T|S)$$\n",
    "\n",
    "We can rewrite the conditional $P(T|S) = \\frac{P(T,S)}{P(S)}$ and therefore achieve:\n",
    "\n",
    "$$T^*(S) = argmax_T \\ \\frac{P(T,S)}{P(S)}$$\n",
    "\n",
    "But from before we realized that $P(T,S)$ is equal to $P(T)$ and for every tree the probability $P(S)$ will be the same, which means it is irrelevant for our purpose.\n",
    "\n",
    "$$T^*(S) = argmax_T \\ P(T)$$\n",
    "\n",
    "This means it is enough to simply choose the parse tree with the highest probability to generate the most likely parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "* Dependency Parsing relies on __Dependency Grammars__ \n",
    "* Consituency parsing relies on __Context Free Grammars__\n",
    "\n",
    "Idea:\n",
    "* Phrase structure is not important\n",
    "* Syntatic structure is important\n",
    "\n",
    "__Typed Dependency__\n",
    "* Dependencies between words are of a sepcific class\n",
    "    * ex det, root, nsub, nmod\n",
    "* The structure of a sentence is with directions between the lexical items (words)\n",
    "\n",
    "__Free word order__\n",
    "Some language have a very relaxed rule-set when it comes to ordering\n",
    "* This mean many CFG rules would be needed, which makes it infeasible\n",
    "* Dependency Grammars has 1 relation per word, pointing to another lexical item, no matter the language\n",
    "\n",
    "__Grammatical Relation (Binary relations)__\n",
    "* Head: Primary noun in NounPhrase or verb in VerbPhrase\n",
    "* Dependent: In DG, head dependent relatonship arises from links between head and word immeadiately dependent on the head\n",
    "\n",
    "Grammatical Function\n",
    "* Role of the dependent, relative to the head\n",
    "* Subject, direct object, indeirect object\n",
    "* In eglish, strongly correlated with word position\n",
    "    * Not in many other languages\n",
    "    \n",
    "Dependency Parsing Formalism\n",
    "* Model as Directed Graph: $G = (V,A) \\quad V: vertices, \\ A: arcs$\n",
    "* $V:$ words, stems, affixes, punctuation\n",
    "* $A:$ Grammatical function relationships\n",
    "\n",
    "\n",
    "Dependency Tree Constraints\n",
    "* One root node - with no incoming arcs\n",
    "* Each node has exactly 1 incoming arc (except root)\n",
    "* There exists a unique path from the root to all other vertices\n",
    "\n",
    "Projectivity\n",
    "* An arc is projective iff there exists a path from head to every word between the head and dependent\n",
    "* A Dependency Tree is projective iff all arcs are projective\n",
    "    * I.e. no crossing arcs\n",
    "* Flexible word order languages = non projective tree\n",
    "* CFGs = Projective Tree\n",
    "\n",
    "Dependency parsing\n",
    "* Lexical head: N = head(NP) and V = head(VP) \n",
    "* Head is the most important word in a phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "__What makes dependency parsing better than constituency parsing when dealing with languages with flexible\n",
    "word orders?__\n",
    "* Constituency parsing requires rules for all word orders in the form of a CFG, whereas Dep. Parsing uses one single head-dep relation which encapsulates all possible word orderings\n",
    "\n",
    "__What are the characteristics of the parses generated through dependency parsing that make them more suitable for tasks such as coreference resolution or question answering?__\n",
    "* Finds HD relationships, whereas const. parsing requires these relationships to be given beforehand.\n",
    "\n",
    "__What are the three restrictions that apply to dependency trees?__\n",
    "* Excatly one root node with no incoming arcs\n",
    "* All nodes exept for the root node has exactly 1 incoming arc\n",
    "* There is a path from the root node to every other node in the graph\n",
    "\n",
    "__An additional constraint is applied to dependency trees, projectivity. What does it mean and why is it important?__\n",
    "What is it?\n",
    "* Projectivity: Phrase is dependent iff there is a path through every word between Head and Dependent.\n",
    "    * A phrase is projective if no arcs are crossing, when set up in the sentence order.\n",
    "    * A tree conisting of only projective phrases is said to be projective\n",
    "Why is it important?\n",
    "* Transition based parsing produces projective trees, and non-projective trees are errorneous\n",
    "* English dependency treebanks were derived from phrase-structure treebanks through the use of head-finding rules, which are projective.\n",
    "\n",
    "__There are two dominant approaches for dependency parsing, transition-based and graph-based. What are their main advantages and disadvantages?__\n",
    "* Transition based: Linear time wrt. to word count, greedy based algorithm (except for beam-search)\n",
    "* Graph-based: Exhaustive search, much slower\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
