{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Theory for Language\n",
    "* Text = String = Sequence of Chars\n",
    "* What is the # bits per character needed to encode English?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "$$H(X) = - \\sum_{x \\in X} p(x) \\cdot \\text{log} \\ p(x)$$\n",
    "\n",
    "Example horse race, all horses uniformly likely $p(x_i) = 1/8$\n",
    "Therefore $H(X) = - \\sum_{i=1}^8 \\frac{1}{8} \\text{log} \\ \\frac{1}{8} = 3$\n",
    "\n",
    "_This means that given that all outcomes are equally likely, we will need 3 bits in order to convey the average value (3 bits allows us to represent 8 diff. numbers)_\n",
    "\n",
    "However, say we have the following probability distr.\n",
    "* $p(x_1) = 1/2$\n",
    "* $p(x_2) = 1/4$\n",
    "* $p(x_3) = 1/8$\n",
    "* $p(x_4) = 1/16$\n",
    "* $p(x_5) = p(x_6) = p(x_7) = p(x_8) = 1/64$\n",
    "\n",
    "Then then Entropy becomes:\n",
    "\n",
    "$H(X) = \\frac{1}{2} \\text{log} \\frac{1}{2} + \\frac{1}{4} \\text{log} \\frac{1}{4} + \\frac{1}{8} \\text{log} \\frac{8}{2}+ \\frac{1}{16} \\text{log} \\frac{1}{16} + 4\\cdot(\\frac{1}{64} \\text{log} \\frac{1}{64}) = 2$\n",
    "\n",
    "Thus to convey the message, we on average only need 2 - if the distribution is heavily in favor of certain outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Equally likely\n",
    "X = np.ones(8) / [8,8,8,8,8,8,8,8]\n",
    "print(sum(-X * np.log2(X)))\n",
    "\n",
    "X = np.ones(8) / [2,4,8,16,64,64,64,64]\n",
    "print(sum(-X * np.log2(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy over a __sequence__\n",
    "\n",
    "$$H(w_1,w_2,...,w_n) = -\\sum_W p(W) \\ \\text{log} \\ p(W)$$\n",
    "\n",
    "__Entropy Rate $H(L)$__\n",
    "\n",
    "$$H(L) := \\text{lim}_{n \\rightarrow \\infty} \\ - \\frac{1}{n} \\ \\text{log} \\ p(w_1 w_2 ... w_n)$$\n",
    "\n",
    "Shannon-McMillan-Breimain theorem:\n",
    "\n",
    "$H(L) = - \\text{lim}_{n \\rightarrow \\infty} \\ \\frac{1}{n} \\sum_{W \\in L} p(w_1,...,w_n) \\cdot \\text{log} \\ p(w_1,...,w_n)$\n",
    "\n",
    "__Cross Entropy $H(p,m)$__\n",
    "\n",
    "Upper bound on the Entropy:\n",
    "\n",
    "$$H(p) \\leq H(p,m)$$\n",
    "\n",
    "__Idea__: Estimate the true Entropy by using a very long sequence, rather than summing over _all_ possible sequences in some language $L$.\n",
    "\n",
    "Useful when we dont know the probability distribution $p$ which generated the data.\n",
    "\n",
    "Instead, use $m$ as the approximation of the unknown distribution: $m \\approx p$ where the CE of $m$ is\n",
    "\n",
    "$$H(p,m) = \\text{lim}_{n \\rightarrow \\infty} \\ - \\frac{1}{n} \\sum_{W \\in L} p(w_1,...,w_n) \\cdot log \\ m(w_1,...,w_n)$$\n",
    "\n",
    "Using the SMB theorem we can simplify this to only depend on $m$:\n",
    "\n",
    "$$H(p,m) = \\text{lim}_{n \\rightarrow \\infty} \\ - \\frac{1}{n} log \\ m(w_1,...,w_n)$$\n",
    "\n",
    "#### Cross Entropy and Perplexity\n",
    "We need an estimate of the cross entropy we can actually work with. Given a sequence of words $W$ the CE approximation is:\n",
    "\n",
    "$$H(W) = - \\frac{1}{N} \\text{log} \\ P(w_1 w_2 ... w_N)$$\n",
    "\n",
    "Perplexity Definition: \n",
    "\n",
    "$$Perp(W) = 2^{H(W)}$$\n",
    "\n",
    "$$= P(w_1 w_2 ... w_N) ^{-1/2}$$ \n",
    "\n",
    "Since we used log2, where $2 ^ {log_2 \\ x} = x$\n",
    "\n",
    "$$= (\\prod_{i=1}^N P(w_i | w_1 ... w_{i-1}))^{-1/N}$$\n",
    "\n",
    "$$= \\sqrt[N]{\\frac{1}{\\prod_{i=1}^N P(w_i | w_1 ... w_{i-1})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity for Uniform Distribution\n",
    "If the distribution is uniform then $Perp(X) = \\vert X \\vert$\n",
    "\n",
    "Proof: \n",
    "\n",
    "Let $X=[x_1...x_N]$ be the random variables and as such the size of $X$ is N and $p(x)=\\frac{1}{N}$ for all $x\\in X$.\n",
    "\n",
    "The entropy of $X$ is then:\n",
    "\n",
    "$$H(X) = -\\sum_{i=1}^N p(x_i) \\cdot \\text{log}_2 p(x_i)$$\n",
    "$$H(X) = - N \\cdot (\\frac{1}{N} \\cdot \\text{log}_2 \\frac{1}{N}) = - \\text{log}_2 \\frac{1}{N} = \\text{log}_2 N - \\text{log}_2 1 = \\text{log}_2 N$$\n",
    "\n",
    "(Since $\\text{log}_2 1 = 0)\n",
    "\n",
    "The perplexity of $X$ then becomes:\n",
    "\n",
    "$$2^ {\\text{log}_2 N} = N $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Models\n",
    "Goal: Predict next word in sequence by using the most probably word $w_i$ given some sequence $w_1 ... w_{i-1}$\n",
    "\n",
    "\n",
    "### Probability\n",
    "Probability for a sequence $P(w_1, w_2, ..., w_n)$\n",
    "\n",
    "Notation: Let the sequence $w_1...w_n = w_1^n$\n",
    "\n",
    "Chain rule for a sequence:\n",
    "$P(w_1 ... w_n) = P(w_1) P(w_2|X_1) P(w_3 | w_1^2) ... P(w_n | w_1^{n-1})$\n",
    "\n",
    "$P(w_1 ... w_n) = \\prod_{i=1}^n P(w_i | w_i^{i-1})$\n",
    "\n",
    "### N-gram models\n",
    "__Bigram model (n=2)__\n",
    "Markov assumption: The probability of a word $w_i$ only depends on the previous word $w_{i-1}$\n",
    "\n",
    "$$P(w_i | w_1^{i-1}) \\approx P(w_i | w_{i-1})$$\n",
    "\n",
    "__N-Gram model__\n",
    "The bigram model can be generalized to using the previous $n$ words to predict the next word, this is called an __N-gram model__, where $N=n+1$\n",
    "\n",
    "$$P(w_i | w_{1}^{i-1}) \\approx P(w_i | w_{i-n}^{i-1})$$\n",
    "\n",
    "### Estimating Probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones(10000)\n",
    "X = X / len(H)\n",
    "\n",
    "def H(X):\n",
    "    return np.sum(-X * np.log2(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999.99999999997"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2** H(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999.999999999995"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(np.log2(10000) -np.log2(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
