{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Theory for Language\n",
    "* Text = String = Sequence of Chars\n",
    "* What is the # bits per character needed to encode English?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "$$H(X) = - \\sum_{x \\in X} p(x) \\cdot \\text{log} \\ p(x)$$\n",
    "\n",
    "Example horse race, all horses uniformly likely $p(x_i) = 1/8$\n",
    "Therefore $H(X) = - \\sum_{i=1}^8 \\frac{1}{8} \\text{log} \\ \\frac{1}{8} = 3$\n",
    "\n",
    "_This means that given that all outcomes are equally likely, we will need 3 bits in order to convey the average value (3 bits allows us to represent 8 diff. numbers)_\n",
    "\n",
    "However, say we have the following probability distr.\n",
    "* $p(x_1) = 1/2$\n",
    "* $p(x_2) = 1/4$\n",
    "* $p(x_3) = 1/8$\n",
    "* $p(x_4) = 1/16$\n",
    "* $p(x_5) = p(x_6) = p(x_7) = p(x_8) = 1/64$\n",
    "\n",
    "Then then Entropy becomes:\n",
    "\n",
    "$H(X) = \\frac{1}{2} \\text{log} \\frac{1}{2} + \\frac{1}{4} \\text{log} \\frac{1}{4} + \\frac{1}{8} \\text{log} \\frac{8}{2}+ \\frac{1}{16} \\text{log} \\frac{1}{16} + 4\\cdot(\\frac{1}{64} \\text{log} \\frac{1}{64}) = 2$\n",
    "\n",
    "Thus to convey the message, we on average only need 2 - if the distribution is heavily in favor of certain outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Equally likely\n",
    "X = np.ones(8) / [8,8,8,8,8,8,8,8]\n",
    "print(sum(-X * np.log2(X)))\n",
    "\n",
    "X = np.ones(8) / [2,4,8,16,64,64,64,64]\n",
    "print(sum(-X * np.log2(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy over a __sequence__\n",
    "\n",
    "$$H(w_1,w_2,...,w_n) = -\\sum_W p(W) \\ \\text{log} \\ p(W)$$\n",
    "\n",
    "__Entropy Rate $H(L)$__\n",
    "\n",
    "$$H(L) := \\text{lim}_{n \\rightarrow \\infty} \\ - \\frac{1}{n} \\ \\text{log} \\ p(w_1 w_2 ... w_n)$$\n",
    "\n",
    "Shannon-McMillan-Breimain theorem:\n",
    "\n",
    "$H(L) = - \\text{lim}_{n \\rightarrow \\infty} \\ \\frac{1}{n} \\sum_{W \\in L} p(w_1,...,w_n) \\cdot \\text{log} \\ p(w_1,...,w_n)$\n",
    "\n",
    "__Cross Entropy $H(p,m)$__\n",
    "\n",
    "Upper bound on the Entropy:\n",
    "\n",
    "$$H(p) \\leq H(p,m)$$\n",
    "\n",
    "__Idea__: Estimate the true Entropy by using a very long sequence, rather than summing over _all_ possible sequences in some language $L$.\n",
    "\n",
    "Useful when we dont know the probability distribution $p$ which generated the data.\n",
    "\n",
    "Instead, use $m$ as the approximation of the unknown distribution: $m \\approx p$ where the CE of $m$ is\n",
    "\n",
    "$$H(p,m) = \\text{lim}_{n \\rightarrow \\infty} \\ - \\frac{1}{n} \\sum_{W \\in L} p(w_1,...,w_n) \\cdot log \\ m(w_1,...,w_n)$$\n",
    "\n",
    "Using the SMB theorem we can simplify this to only depend on $m$:\n",
    "\n",
    "$$H(p,m) = \\text{lim}_{n \\rightarrow \\infty} \\ - \\frac{1}{n} log \\ m(w_1,...,w_n)$$\n",
    "\n",
    "#### Cross Entropy and Perplexity\n",
    "We need an estimate of the cross entropy we can actually work with. Given a sequence of words $W$ the CE approximation is:\n",
    "\n",
    "$$H(W) = - \\frac{1}{N} \\text{log} \\ P(w_1 w_2 ... w_N)$$\n",
    "\n",
    "Perplexity Definition: \n",
    "\n",
    "$$Perp(W) = 2^{H(W)}$$\n",
    "\n",
    "$$= P(w_1 w_2 ... w_N) ^{-1/2}$$ \n",
    "\n",
    "Since we used log2, where $2 ^ {log_2 \\ x} = x$\n",
    "\n",
    "$$= (\\prod_{i=1}^N P(w_i | w_1 ... w_{i-1}))^{-1/N}$$\n",
    "\n",
    "$$= \\sqrt[N]{\\frac{1}{\\prod_{i=1}^N P(w_i | w_1 ... w_{i-1})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity for Uniform Distribution\n",
    "If the distribution is uniform then $Perp(X) = \\vert X \\vert$\n",
    "\n",
    "Proof: \n",
    "\n",
    "Let $X=[x_1...x_N]$ be the random variables and as such the size of $X$ is N and $p(x)=\\frac{1}{N}$ for all $x\\in X$.\n",
    "\n",
    "The entropy of $X$ is then:\n",
    "\n",
    "$$H(X) = -\\sum_{i=1}^N p(x_i) \\cdot \\text{log}_2 p(x_i)$$\n",
    "$$H(X) = - N \\cdot (\\frac{1}{N} \\cdot \\text{log}_2 \\frac{1}{N}) = - \\text{log}_2 \\frac{1}{N} = \\text{log}_2 N - \\text{log}_2 1 = \\text{log}_2 N$$\n",
    "\n",
    "(We used $\\text{log}_2 1 = 0$)\n",
    "\n",
    "The perplexity of $X$ then becomes:\n",
    "\n",
    "$$2^ {\\text{log}_2 N} = N \\quad \\square$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating language models\n",
    "Extrinsic evaluation: Compare accuracy, ex run two language models over a text and see which one has the higher accuracy. Usually very expensive and time consuming.\n",
    "\n",
    "Intrinsic evaluation: Quality of model independent of application. Whichever model assigns the highest probability to the test set is the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Models\n",
    "Goal: Predict next word in sequence by using the most probably word $w_i$ given some sequence $w_1 ... w_{i-1}$\n",
    "\n",
    "\n",
    "### Probability\n",
    "Probability for a sequence $P(w_1, w_2, ..., w_n)$\n",
    "\n",
    "Notation: Let the sequence $w_1...w_n = w_1^n$\n",
    "\n",
    "Chain rule for a sequence:\n",
    "$P(w_1 ... w_n) = P(w_1) P(w_2|X_1) P(w_3 | w_1^2) ... P(w_n | w_1^{n-1})$\n",
    "\n",
    "$P(w_1 ... w_n) = \\prod_{i=1}^n P(w_i | w_i^{i-1})$\n",
    "\n",
    "### N-gram models\n",
    "__Bigram model (n=2)__\n",
    "Markov assumption: The probability of a word $w_i$ only depends on the previous word $w_{i-1}$\n",
    "\n",
    "$$P(w_i | w_1^{i-1}) \\approx P(w_i | w_{i-1})$$\n",
    "\n",
    "__N-Gram model__\n",
    "The bigram model can be generalized to using the previous $n$ words to predict the next word, this is called an __N-gram model__, where $N=n+1$\n",
    "\n",
    "$$P(w_i | w_{1}^{i-1}) \\approx P(w_i | w_{i-n}^{i-1})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Probabilities\n",
    "Model: $p(w_i | w_1^{i-1}) \\approx p(w_i | w_{i-n}^{i-1}$\n",
    "\n",
    "Empirically: $$\\frac{\\text{count}(w_{i-n}^n)}{\\sum_{w'} \\text{count} (w_{i-n}^{i-1}, w')} = \\frac{\\text{count}(w_{i-n}^n)}{\\text{count} (w_{i-n}^{i-1})}$$\n",
    "\n",
    "#### Bigram example\n",
    "Example on the following corpus:\n",
    "```\n",
    "<S> I am Sam </S>\n",
    "<S> Sam I am </S>\n",
    "<S> I do not like green eggs and ham </S>\n",
    "```\n",
    "\n",
    "Using a bigram model the probabilities of words becomes:\n",
    "* p( I | `<S>`)= 2/3\n",
    "* p(Sam | `<S>`) = 1/3\n",
    "* p(am | I ) = 2/3\n",
    "* p(`</S>` | Sam) = 1/3\n",
    "* p(Sam | am) = 1/2\n",
    "* p(do | I) = 1/3\n",
    "* p(I | Sam) = 1/2\n",
    "* p(`</S>`|`am`) = 1/2\n",
    "\n",
    "Probability for a sentence:\n",
    "\n",
    "p(`<S> Sam I am </S>`) = p(`Sam`|`<S>`) p(`I`|`Sam`) p(`am`|`I`) p(`</S>`|`am`)\n",
    "\n",
    "$ = \\frac{1}{3} \\cdot \\frac{1}{2} \\cdot \\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{2}{36} = \\frac{1}{18}$\n",
    "#### N-Gram example\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown Words\n",
    "Problem: We need a fixed vocabulary for basic language models unknown word are words not in the vocabulary. \n",
    "\n",
    "What is the probability of seen an unknwon word next?\n",
    "\n",
    "Idea: Treat all unknwon words in training corpus the same, assign then $UNK$ token to them and all future unknwon words are then also treated as the $UNK$ token.\n",
    "\n",
    "Problem: By choosing a small vocab, and thus making a lot of words $UNK$ the perplexity score will be low, since the model will just predict that it doesn't know the next word when predicting - which is a really bad model.\n",
    "\n",
    "\n",
    "### Smoothing\n",
    "Problem: Words that are in corpus, but rarely occur, will have n-gram probabilities of zero, destroying the probabilistic modelling.\n",
    "\n",
    "Idea: Smoothe the probability distribution slightly in favor of rarely seen n-grams/words, i.e. take some mass away from frequent words and give it to infrequent words.\n",
    "\n",
    "#### Laplace/Add-One Smoothing\n",
    "Actually bad, but good as a baseline\n",
    "\n",
    "Idea: For a vocab of size $V$\n",
    "* Unsmoothed n-gram probability: $$P(w_n | w_{n-1}) = \\frac{\\text{count}(w_{i-n}^n)}{\\text{count} (w_{i-n}^{i-1})}$$\n",
    "* Laplace smoothed n-gram probability: $$P(w_n | w_{n-1}) = \\frac{\\text{count}(w_{i-n}^n) + 1}{\\text{count} (w_{i-n}^{i-1}) +V}$$\n",
    "\n",
    "However for many zero counts this method can really mess us the distribution.\n",
    "\n",
    "Idea. Can be generalized to add-k smoothing, where $k \\in [0;1]$ rather than 1.\n",
    "\n",
    "\n",
    "$$P(w_n | w_{n-1}) = \\frac{\\text{count}(w_{i-n}^n) + k}{\\text{count} (w_{i-n}^{i-1}) +kV}$$\n",
    "\n",
    "#### Good-Turing Estimator\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backoff and Interpolation\n",
    "Rather than using a constant frequency for all missing n-grams, we can insead make a qualified estimation by using the probability of an (n-1)-gram.\n",
    "\n",
    "Example estimating a trigram count from the bigram:\n",
    "$$P(w_n | w_{n-2} w_{n-1}) \\approx P(w_n | w_{n-1})$$\n",
    "\n",
    "Similarily we can estimate bigrams via unigrams (word count):\n",
    "$$P(w_n |w_{n-1}) \\approx P(w_n)$$\n",
    "\n",
    "__Backoff__\n",
    "\n",
    "Only use n-gram if evidence i sufficient, i.e. nonzero\n",
    "* Otherwise try trigram, bigram, unigram...\n",
    "\n",
    "__Interpolation__\n",
    "\n",
    "Mix probability estimates from all lower n-gram estimaters and weight the counts\n",
    "\n",
    "$$P(w_n | w_{n-2} w_{n-1}) \\approx \\lambda_1 P(w_n | w_{n-2} w_{n-1}) + \\lambda_2 P(w_n | w_{n-1}) + \\lambda_3 P(w_n) $$\n",
    "\n",
    "$$\\sum_i \\lambda_i = 1$$\n",
    "\n",
    "\n",
    "#### Backoff with Discounting (Katz Backoff)\n",
    "Discount higher-order n-grams in order to transder probability mass to the lower order n-grams, this is called the __Katz Backoff__, denoted $P^*$,\n",
    "\n",
    "If not discounted then replacing 0 probability n-gram with a >0 probability lower-order n-gram, the total probability mass would exceed 1.\n",
    "\n",
    "Concretely discount probability if seem before (i.e. nonzero counts) otherwise, recursively back off to lower-order n-gram prob. $P^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Models\n",
    "Vocab $V$ of size $m$, embedding dimension of $d$\n",
    "\n",
    "__word embeddings (vector encodings)__ \n",
    "\n",
    "Indicator/one hot encoding of a word: $w \\in (0,1) ^m$\n",
    "\n",
    "Word embedding: $x: w \\rightarrow x_w \\quad x_w \\in R^d$\n",
    "\n",
    "__Sequence embedding__\n",
    "\n",
    "Concatenate word embedded vectors $s = [x_{w_1} ... x_{w_n}]$ such that $s \\in R ^{nd}$\n",
    "\n",
    "__Conv layers__\n",
    "Window size of filter, ex $w=3$\n",
    "\n",
    "For a single channel $f_j : R^{3d} \\rightarrow R$\n",
    "* Apply filter to input\n",
    "* Parameter sharing for filters\n",
    "* Apply activation function in the end\n",
    "\n",
    "For $k$ independent channels\n",
    "* Use padding\n",
    "* Compute $n\\times k$ numbers\n",
    "\n",
    "__Pooling layers__\n",
    "\n",
    "No parameters!\n",
    "\n",
    "Goal: Reduce each channel output to a single number: $R^{n\\times k} \\rightarrow R^k$\n",
    "* Typically max-over-time pooling\n",
    "* Condense history into vector $h \\rightarrow z_h \\in R^k$\n",
    "\n",
    "__Softmax__\n",
    "Convert to word-probabilities\n",
    "\n",
    "Each word having an associated weight-vector $\\beta_w \\in R^k$\n",
    "\n",
    "$$p(w |h) = \\frac{exp(\\langle \\beta_w, z_h \\rangle)}{\\sum_{w'} exp(\\langle \\beta_{w'}, z_h \\rangle}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
